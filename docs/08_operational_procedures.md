# Operational Procedures ‚Äî VoP –°–ï–ü –ù–ë–£

**–í–µ—Ä—Å—ñ—è:** 1.0
**–î–∞—Ç–∞:** 2026-02-07
**–°—Ç–∞—Ç—É—Å:** Final

---

## –ó–º—ñ—Å—Ç

1. [–û–≥–ª—è–¥](#–æ–≥–ª—è–¥)
2. [–û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞](#–æ—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π–Ω–∞-—Å—Ç—Ä—É–∫—Ç—É—Ä–∞)
3. [–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ 24/7](#–º–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥-247)
4. [Incident Response](#incident-response)
5. [Change Management](#change-management)
6. [Backup —Ç–∞ Recovery](#backup-—Ç–∞-recovery)
7. [Security Operations](#security-operations)
8. [Capacity Planning](#capacity-planning)
9. [Runbooks](#runbooks)
10. [–ö–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–∞ –µ—Å–∫–∞–ª–∞—Ü—ñ—è](#–∫–æ–Ω—Ç–∞–∫—Ç–∏-—Ç–∞-–µ—Å–∫–∞–ª–∞—Ü—ñ—è)

---

## –û–≥–ª—è–¥

–¶–µ–π –¥–æ–∫—É–º–µ–Ω—Ç –æ–ø–∏—Å—É—î –æ–ø–µ—Ä–∞—Ü—ñ–π–Ω—ñ –ø—Ä–æ—Ü–µ–¥—É—Ä–∏ –¥–ª—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏ —Å–∏—Å—Ç–µ–º–∏ Verification of Payee (VoP) –≤ production environment.

**–¶—ñ–ª—å–æ–≤–∞ –∞—É–¥–∏—Ç–æ—Ä—ñ—è:**
- üë®‚Äçüíº Operations team (24/7 monitoring)
- üë®‚Äçüíª Engineering team (incident response)
- üîê Security team (security incidents)
- üìä Management (escalation)

**Scope:**
- VoP Router (–ù–ë–£)
- VoP Directory Service
- –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Ç–∞ alerting
- Incident management
- Change management
- Disaster recovery

---

## –û—Ä–≥–∞–Ω—ñ–∑–∞—Ü—ñ–π–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

### –ö–æ–º–∞–Ω–¥–∏ —Ç–∞ —Ä–æ–ª—ñ

#### 1. Operations Team (24/7)

**–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å:**
- –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º–∏ 24/7
- –†–µ–∞–≥—É–≤–∞–Ω–Ω—è –Ω–∞ alerts
- –ü–µ—Ä—à–∞ –ª—ñ–Ω—ñ—è –ø—ñ–¥—Ç—Ä–∏–º–∫–∏
- –ï—Å–∫–∞–ª–∞—Ü—ñ—è incidents
- –í–µ–¥–µ–Ω–Ω—è incident log

**–°–∫–ª–∞–¥:**
- L1 Support Engineers (6 –æ—Å—ñ–±, —Ä–æ—Ç–∞—Ü—ñ—è –ø–æ 12 –≥–æ–¥–∏–Ω)
- L2 Support Engineers (4 –æ—Å–æ–±–∏, on-call)

**–†–æ–±–æ—á–∏–π —á–∞—Å:**
- 24/7/365
- –ó–º—ñ–Ω–Ω—ñ—Å—Ç—å: 2 –∑–º—ñ–Ω–∏ –ø–æ 12 –≥–æ–¥–∏–Ω (08:00-20:00, 20:00-08:00)
- Weekend coverage: 2 –æ—Å–æ–±–∏ –Ω–∞ –∑–º—ñ–Ω—É

#### 2. Engineering Team

**–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å:**
- –†–æ–∑—Ä–æ–±–∫–∞ —Ç–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ VoP —Å–∏—Å—Ç–µ–º–∏
- Incident resolution (P0, P1)
- Performance optimization
- Code reviews —Ç–∞ deployments
- Technical documentation

**–°–∫–ª–∞–¥:**
- Backend Engineers (4 –æ—Å–æ–±–∏)
- DevOps Engineers (2 –æ—Å–æ–±–∏)
- Security Engineer (1 –æ—Å–æ–±–∞)
- Tech Lead (1 –æ—Å–æ–±–∞)

**–†–æ–±–æ—á–∏–π —á–∞—Å:**
- –ü–Ω-–ü—Ç 09:00-18:00
- On-call rotation (24/7 –¥–ª—è P0/P1)

#### 3. Security Team

**–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å:**
- Security monitoring
- Incident response (security breaches)
- Certificate management
- Vulnerability assessments
- Compliance audits

**–°–∫–ª–∞–¥:**
- Security Engineers (2 –æ—Å–æ–±–∏)
- Security Analyst (1 –æ—Å–æ–±–∞)

#### 4. Management

**–í—ñ–¥–ø–æ–≤—ñ–¥–∞–ª—å–Ω—ñ—Å—Ç—å:**
- –ï—Å–∫–∞–ª–∞—Ü—ñ—è critical incidents
- –ö–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è –∑—ñ stakeholders
- Budget approval
- Strategic decisions

**–°–∫–ª–∞–¥:**
- Head of IT Department
- VoP Product Owner
- Service Delivery Manager

---

## –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ 24/7

### –ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ —ñ–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∏

#### Prometheus + Grafana

**Metrics:**

```prometheus
# VoP Router
vop_router_requests_total
vop_router_request_duration_seconds
vop_router_errors_total
vop_router_active_requests

# Database
pg_up
pg_stat_database_numbackends
pg_stat_database_xact_commit
pg_stat_database_blks_hit

# Redis
redis_up
redis_connected_clients
redis_used_memory_bytes

# System
node_cpu_seconds_total
node_memory_MemAvailable_bytes
node_disk_io_time_seconds_total
```

**Dashboards:**

1. **VoP Overview Dashboard**
   - Requests per second (—É—Å–ø—ñ—à–Ω—ñ / –ø–æ–º–∏–ª–∫–∏)
   - Latency (p50, p95, p99)
   - Error rate %
   - Active requests
   - Match status distribution

2. **Infrastructure Dashboard**
   - CPU usage (per pod)
   - Memory usage
   - Disk I/O
   - Network traffic

3. **Database Dashboard**
   - Query performance
   - Connection pool
   - Slow queries (> 100ms)
   - Database size

4. **Redis Dashboard**
   - Connected clients
   - Memory usage
   - Cache hit rate
   - Evictions

**Grafana URL:** https://monitoring.nbu.gov.ua/grafana

### Alerting

#### AlertManager configuration

**Critical Alerts (PagerDuty):**

```yaml
groups:
- name: vop_critical
  rules:
  # VoP Router down
  - alert: VopRouterDown
    expr: up{job="vop-router"} == 0
    for: 1m
    severity: critical
    annotations:
      summary: "VoP Router is down"
      description: "VoP Router has been down for more than 1 minute"

  # High error rate
  - alert: VopHighErrorRate
    expr: rate(vop_router_errors_total[5m]) > 0.05
    for: 5m
    severity: critical
    annotations:
      summary: "VoP error rate > 5%"

  # High latency
  - alert: VopHighLatency
    expr: histogram_quantile(0.95, vop_router_request_duration_seconds) > 1.0
    for: 5m
    severity: critical
    annotations:
      summary: "VoP p95 latency > 1 second"

  # Database down
  - alert: DatabaseDown
    expr: pg_up == 0
    for: 1m
    severity: critical

  # Redis down
  - alert: RedisDown
    expr: redis_up == 0
    for: 1m
    severity: critical
```

**Warning Alerts (Slack):**

```yaml
- name: vop_warnings
  rules:
  # Elevated error rate
  - alert: VopElevatedErrorRate
    expr: rate(vop_router_errors_total[5m]) > 0.02
    for: 10m
    severity: warning

  # Elevated latency
  - alert: VopElevatedLatency
    expr: histogram_quantile(0.95, vop_router_request_duration_seconds) > 0.5
    for: 10m
    severity: warning

  # High CPU
  - alert: HighCPUUsage
    expr: rate(node_cpu_seconds_total{mode="idle"}[5m]) < 0.2
    for: 10m
    severity: warning

  # Low disk space
  - alert: LowDiskSpace
    expr: node_filesystem_avail_bytes / node_filesystem_size_bytes < 0.1
    for: 5m
    severity: warning
```

**Notification channels:**

- **Critical:** PagerDuty (SMS + Phone call)
- **Warning:** Slack #vop-alerts
- **Info:** Slack #vop-monitoring

### Log Monitoring (ELK Stack)

**Elasticsearch queries:**

```json
# Error logs
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "error" } },
        { "range": { "@timestamp": { "gte": "now-15m" } } }
      ]
    }
  }
}

# Slow requests (> 1 sec)
{
  "query": {
    "range": {
      "duration": { "gte": 1000 }
    }
  }
}

# Failed authentication
{
  "query": {
    "match": { "message": "authentication failed" }
  }
}
```

**Kibana dashboards:**

1. Error Log Dashboard
2. Request Log Dashboard
3. Security Log Dashboard
4. Audit Log Dashboard

**Kibana URL:** https://monitoring.nbu.gov.ua/kibana

### On-Call Schedule

**PagerDuty rotation:**

```
Week 1: Engineer A (primary), Engineer B (secondary)
Week 2: Engineer C (primary), Engineer D (secondary)
Week 3: Engineer B (primary), Engineer A (secondary)
Week 4: Engineer D (primary), Engineer C (secondary)
```

**On-call responsibilities:**

- ‚úÖ Respond to P0/P1 incidents within 15 minutes
- ‚úÖ –ï—Å–∫–∞–ª–∞—Ü—ñ—è –¥–æ Tech Lead —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
- ‚úÖ Post-incident report –ø—Ä–æ—Ç—è–≥–æ–º 24 –≥–æ–¥–∏–Ω
- ‚úÖ –î–æ—Å—Ç—É–ø–Ω—ñ—Å—Ç—å: phone + laptop + internet

**Compensation:**

- On-call week: +20% –¥–æ –∑–∞—Ä–ø–ª–∞—Ç–∏
- Incident response (non-business hours): +50% hourly rate

---

## Incident Response

### Incident Severity Levels

| Severity | Description | Response Time | Escalation |
|----------|-------------|---------------|------------|
| **P0 (Critical)** | VoP Router down, data breach | 15 min | Immediate (Tech Lead + Management) |
| **P1 (High)** | High error rate (> 5%), high latency | 30 min | Tech Lead |
| **P2 (Medium)** | Elevated errors, performance degradation | 2 hours | Engineering team |
| **P3 (Low)** | Minor issues, cosmetic bugs | 1 business day | Normal process |

### P0 Incident Response

**–ü—Ä–∏–∫–ª–∞–¥: VoP Router –ø–æ–≤–Ω—ñ—Å—Ç—é –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π**

**–ö—Ä–æ–∫ 1: Detection (0-5 min)**

1. AlertManager trigger: `VopRouterDown`
2. PagerDuty phone call –¥–æ on-call engineer
3. Slack alert —É #vop-critical

**–ö—Ä–æ–∫ 2: Initial Response (5-15 min)**

1. On-call engineer acknowledge incident —É PagerDuty
2. –ü–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ status:
   ```bash
   kubectl get pods -n vop
   kubectl logs -n vop deployment/vop-router --tail=100
   ```
3. –°—Ç–≤–æ—Ä–∏—Ç–∏ incident —É JIRA: `INC-YYYYMMDD-NNN`
4. –ü–æ–≤—ñ–¥–æ–º–∏—Ç–∏ —É Slack #vop-incidents:
   ```
   üö® P0 INCIDENT: VoP Router Down
   Incident ID: INC-20260207-001
   Detected: 2026-02-07 14:35 UTC
   On-call: Engineer A
   Status: Investigating
   ```

**–ö—Ä–æ–∫ 3: Investigation (15-30 min)**

1. Check infrastructure:
   ```bash
   # Kubernetes cluster health
   kubectl get nodes
   kubectl top nodes

   # Pod status
   kubectl describe pod -n vop vop-router-xxxxx

   # Recent events
   kubectl get events -n vop --sort-by='.lastTimestamp'
   ```

2. Check dependencies:
   - PostgreSQL: `psql -h db-host -U vop_user -c "SELECT 1"`
   - Redis: `redis-cli -h redis-host PING`

3. Check logs:
   - Application logs (Kibana)
   - System logs (`journalctl`)
   - Kubernetes logs

**–ö—Ä–æ–∫ 4: Escalation (—è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ)**

–Ø–∫—â–æ –Ω–µ –º–æ–∂–Ω–∞ –≤–∏—Ä—ñ—à–∏—Ç–∏ –∑–∞ 30 —Ö–≤–∏–ª–∏–Ω:

1. Escalate –¥–æ Tech Lead (phone call)
2. Escalate –¥–æ DevOps team (—è–∫—â–æ infrastructure issue)
3. Escalate –¥–æ Management (—è–∫—â–æ > 1 –≥–æ–¥–∏–Ω–∞ downtime)

**–ö—Ä–æ–∫ 5: Resolution**

–ü—Ä–∏–∫–ª–∞–¥ –ø—Ä–∏—á–∏–Ω —Ç–∞ —Ä—ñ—à–µ–Ω—å:

| –ü—Ä–∏—á–∏–Ω–∞ | –†—ñ—à–µ–Ω–Ω—è |
|---------|---------|
| Pod crashed (OOMKilled) | –ó–±—ñ–ª—å—à–∏—Ç–∏ memory limit, restart pod |
| Database connection pool exhausted | –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–∏ DB connections, scale up |
| Kubernetes node down | Cordon node, drain pods, provision new node |
| Certificate expired | Renew certificate, redeploy |
| DDoS attack | Enable rate limiting, block IPs |

```bash
# Restart pods
kubectl rollout restart deployment/vop-router -n vop

# Scale up
kubectl scale deployment/vop-router --replicas=5 -n vop

# Check rollout status
kubectl rollout status deployment/vop-router -n vop
```

**–ö—Ä–æ–∫ 6: Verification**

1. Check health endpoint:
   ```bash
   curl -k https://vop-router.nbu.gov.ua/health
   ```

2. Send test VoP request

3. Monitor metrics (Grafana):
   - Requests per second –≤—ñ–¥–Ω–æ–≤–ª—é—î—Ç—å—Å—è
   - Error rate < 1%
   - Latency –≤ –º–µ–∂–∞—Ö SLA

**–ö—Ä–æ–∫ 7: Communication**

1. Update incident status —É Slack:
   ```
   ‚úÖ RESOLVED: VoP Router Down
   Incident ID: INC-20260207-001
   Root cause: Database connection pool exhausted
   Resolution: Restarted DB connections, scaled to 5 replicas
   Downtime: 23 minutes
   ```

2. Close incident —É PagerDuty

3. Send email –¥–æ stakeholders:
   ```
   Subject: [RESOLVED] VoP Incident - 23 minutes downtime

   Dear stakeholders,

   VoP Router experienced downtime from 14:35 to 14:58 UTC (23 minutes).

   Root cause: Database connection pool was exhausted due to spike in traffic.

   Resolution: We restarted database connections and scaled VoP Router to 5 replicas.

   Impact: Approximately 1,500 VoP requests failed during this period.

   Next steps: We will increase DB connection pool size and add alerting for connection pool usage.

   Full RCA will be published within 48 hours.

   Regards,
   VoP Operations Team
   ```

**–ö—Ä–æ–∫ 8: Post-Incident Review (24-48 hours)**

–°—Ç–≤–æ—Ä–∏—Ç–∏ RCA (Root Cause Analysis) document:

**Template:**

```markdown
# Root Cause Analysis: VoP Router Downtime

**Incident ID:** INC-20260207-001
**Date:** 2026-02-07
**Duration:** 23 minutes (14:35-14:58 UTC)
**Severity:** P0

## Summary

VoP Router was completely unavailable for 23 minutes due to database connection pool exhaustion.

## Timeline

- 14:35 - Alert triggered: VopRouterDown
- 14:37 - On-call engineer acknowledged
- 14:40 - Investigation started
- 14:45 - Root cause identified: DB connection pool exhausted
- 14:50 - DB connections restarted, scaled to 5 replicas
- 14:55 - Service restored
- 14:58 - Verification complete, incident closed

## Root Cause

Database connection pool size was configured to 20 connections. During peak traffic (1,500 req/s), all connections were exhausted, causing new requests to timeout.

## Impact

- 1,500 VoP requests failed (returned HTTP 500)
- Estimated affected payments: 1,500
- No data loss or security breach

## Resolution

1. Restarted database connections
2. Scaled VoP Router from 3 to 5 replicas
3. Increased DB connection pool size from 20 to 50

## Prevention

Action items:

1. [ ] Increase default DB connection pool to 50 (Owner: Engineer A, Due: 2026-02-10)
2. [ ] Add alerting for DB connection pool usage > 80% (Owner: DevOps B, Due: 2026-02-10)
3. [ ] Implement connection pool auto-scaling (Owner: Engineer C, Due: 2026-02-15)
4. [ ] Load testing to identify limits (Owner: QA Team, Due: 2026-02-20)

## Lessons Learned

- Need better monitoring of DB connection pool
- Auto-scaling should consider DB connections, not just CPU/memory
- Traffic spike patterns should trigger proactive scaling
```

### P1 Incident Response

**–ü—Ä–∏–∫–ª–∞–¥: High error rate (10%)**

Process —Å—Ö–æ–∂–∏–π –¥–æ P0, –∞–ª–µ:
- Response time: 30 min
- –ú–µ–Ω—à –∞–≥—Ä–µ—Å–∏–≤–Ω–∞ –µ—Å–∫–∞–ª–∞—Ü—ñ—è
- –ú–æ–∂–Ω–∞ —Å–ø—Ä–æ–±—É–≤–∞—Ç–∏ mitigations –ø–µ—Ä–µ–¥ –ø–æ–≤–Ω–∏–º restart

**Typical P1 scenarios:**
- Elevated error rate (5-15%)
- High latency (p95 > 2 seconds)
- Responder bank unavailable
- Certificate expiring soon (< 7 days)

---

## Change Management

### Change Process

**Types of changes:**

1. **Standard Change** ‚Äî pre-approved, low risk (e.g., configuration update)
2. **Normal Change** ‚Äî requires CAB approval
3. **Emergency Change** ‚Äî critical fix, expedited approval

### Standard Change (Low Risk)

**Examples:**
- Update log level
- Adjust cache TTL
- Minor configuration changes

**Process:**

1. Create change ticket —É JIRA: `CHG-YYYYMMDD-NNN`
2. Document change details:
   - What is changing
   - Why (business justification)
   - Rollback plan
3. Schedule change (prefer off-peak hours)
4. Execute change
5. Verify
6. Close ticket

**Approval:** Tech Lead (can be done asynchronously)

**Timing:** Anytime (avoid peak hours 10:00-16:00)

### Normal Change (Medium/High Risk)

**Examples:**
- Deploy new version
- Database schema change
- Infrastructure upgrade
- Certificate renewal

**Process:**

1. **Request (1 week before)**
   - Create RFC (Request for Change) —É JIRA
   - Fill change template:
     ```
     Title: Deploy VoP Router v1.1.0
     Description: New version with performance improvements
     Risk: Medium
     Impact: Low (rolling deployment, no downtime expected)
     Rollback plan: kubectl rollout undo deployment/vop-router
     Testing: Completed in staging environment
     Deployment plan: Rolling update, 1 pod at a time
     ```

2. **CAB Review (3 days before)**
   - Change Advisory Board (CAB) meeting: Wednesday 14:00
   - Participants: Tech Lead, DevOps Lead, Service Delivery Manager
   - Review RFC
   - Approve / Reject / Request more info

3. **Preparation**
   - Notify stakeholders (email)
   - Update status page
   - Prepare rollback scripts
   - Backup current configuration

4. **Execution (change window)**
   - Preferred time: Tuesday/Wednesday 22:00-02:00 (low traffic)
   - Execute deployment:
     ```bash
     # Deploy new version
     kubectl set image deployment/vop-router \
       vop-router=vop-router:1.1.0 -n vop

     # Monitor rollout
     kubectl rollout status deployment/vop-router -n vop

     # If issues, rollback
     kubectl rollout undo deployment/vop-router -n vop
     ```

5. **Verification**
   - Health checks
   - Smoke tests
   - Monitor metrics (30 minutes)

6. **Post-Change**
   - Update documentation
   - Close RFC
   - Send completion email

**Approval:** CAB (Change Advisory Board)

**Timing:** Maintenance window (Tuesday/Wednesday 22:00-02:00)

### Emergency Change (Critical Fix)

**Examples:**
- Security vulnerability patch
- Critical bug fix
- Certificate expired

**Process:**

1. **Request**
   - Create emergency RFC
   - Notify Management immediately
   - Get verbal approval from Tech Lead or CTO

2. **Expedited Review**
   - Emergency CAB (virtual, within 30 min)
   - Fast-track approval

3. **Execution**
   - Can be done during business hours if critical
   - Notify all stakeholders
   - Execute with extra care

4. **Post-Change**
   - Detailed RCA
   - Update runbooks

**Approval:** CTO or Head of IT

**Timing:** ASAP (–¥–∞–∂–µ –≤ —Ä–æ–±–æ—á–∏–π —á–∞—Å —è–∫—â–æ –∫—Ä–∏—Ç–∏—á–Ω–æ)

### Change Calendar

**Preferred change windows:**

| Day | Time | Type |
|-----|------|------|
| Tuesday | 22:00-02:00 | Normal changes |
| Wednesday | 22:00-02:00 | Normal changes |
| Thursday | 22:00-02:00 | Emergency only |
| Friday | ‚ùå No changes | - |
| Weekend | ‚ùå No changes | Emergency only |

**Blackout periods (no changes):**

- –ü–µ—Ä—à–∏–π —Ä–æ–±–æ—á–∏–π –¥–µ–Ω—å –ø—ñ—Å–ª—è —Å–≤—è—Ç
- –û—Å—Ç–∞–Ω–Ω—ñ–π –¥–µ–Ω—å –º—ñ—Å—è—Ü—è (—Ñ—ñ–Ω–∞–Ω—Å–æ–≤–∞ –∑–≤—ñ—Ç–Ω—ñ—Å—Ç—å)
- –ü—ñ–¥ —á–∞—Å –∞—É–¥–∏—Ç—É

---

## Backup —Ç–∞ Recovery

### Backup Strategy

#### Database (PostgreSQL)

**Backup schedule:**

- **Full backup:** –©–æ–¥–Ω—è –æ 02:00 UTC
- **Incremental backup:** –ö–æ–∂–Ω—ñ 6 –≥–æ–¥–∏–Ω
- **WAL archiving:** Continuous

**Backup script:**

```bash
#!/bin/bash
# /opt/vop/scripts/backup-db.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="/backups/postgres"
DB_NAME="vop_directory"

# Full backup
pg_dump -h db-host -U vop_user -Fc $DB_NAME \
  > $BACKUP_DIR/vop_db_$DATE.dump

# Compress
gzip $BACKUP_DIR/vop_db_$DATE.dump

# Upload to S3
aws s3 cp $BACKUP_DIR/vop_db_$DATE.dump.gz \
  s3://nbu-vop-backups/postgres/

# Cleanup old backups (keep 30 days)
find $BACKUP_DIR -name "*.dump.gz" -mtime +30 -delete
```

**Retention:**

- Local: 7 days
- S3: 30 days
- Long-term (yearly): 7 years

**Backup verification:**

–©–æ—Ç–∏–∂–Ω—è (—Å—É–±–æ—Ç–∞ 04:00) ‚Äî restore test backup —É staging:

```bash
# Restore from latest backup
pg_restore -h staging-db -U vop_user -d vop_directory_test \
  /backups/postgres/vop_db_latest.dump

# Verify data
psql -h staging-db -U vop_user -d vop_directory_test \
  -c "SELECT COUNT(*) FROM vop_directory"
```

#### Redis

**Backup schedule:**

- **RDB snapshot:** –©–æ–≥–æ–¥–∏–Ω–∏
- **AOF:** Enabled (fsync every second)

**Backup location:** `/data/redis-backups/`

**Retention:** 7 days (local), 30 days (S3)

#### Configuration (Git)

–í—Å—ñ –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ–π–Ω—ñ —Ñ–∞–π–ª–∏ –∑–±–µ—Ä—ñ–≥–∞—é—Ç—å—Å—è –≤ Git:

- Kubernetes manifests
- Helm charts
- Environment variables (encrypted –∑ SOPS)
- Scripts

**Repository:** `git@github.com:nbu/vop-infrastructure.git`

### Disaster Recovery

#### RTO and RPO

| System | RTO (Recovery Time Objective) | RPO (Recovery Point Objective) |
|--------|-------------------------------|-------------------------------|
| VoP Router | 1 hour | 15 minutes |
| PostgreSQL | 2 hours | 1 hour |
| Redis | 30 minutes | 1 hour |

#### DR Procedures

**Scenario 1: Kubernetes cluster failure**

1. **Detection:**
   - All pods down
   - Cluster API unreachable

2. **Recovery:**
   ```bash
   # Provision new cluster (Terraform)
   cd /opt/vop/terraform
   terraform apply -target=module.kubernetes_cluster

   # Deploy VoP from Git
   git clone git@github.com:nbu/vop-infrastructure.git
   cd vop-infrastructure/k8s
   kubectl apply -f .

   # Restore database
   pg_restore -h new-db-host -U vop_user -d vop_directory \
     s3://nbu-vop-backups/postgres/latest.dump
   ```

3. **Verification:**
   - Health checks
   - Smoke tests
   - DNS update (if new IPs)

**Estimated time:** 1-2 hours

**Scenario 2: Database corruption**

1. **Detection:**
   - Database errors
   - Data inconsistency

2. **Recovery:**
   ```bash
   # Stop VoP Router
   kubectl scale deployment/vop-router --replicas=0 -n vop

   # Drop corrupted database
   psql -h db-host -U postgres -c "DROP DATABASE vop_directory"

   # Create new database
   psql -h db-host -U postgres -c "CREATE DATABASE vop_directory"

   # Restore from backup
   pg_restore -h db-host -U vop_user -d vop_directory \
     /backups/postgres/vop_db_latest.dump

   # Verify data integrity
   psql -h db-host -U vop_user -d vop_directory \
     -f /opt/vop/scripts/verify-data.sql

   # Start VoP Router
   kubectl scale deployment/vop-router --replicas=3 -n vop
   ```

**Estimated time:** 2-3 hours

**Data loss:** Up to 1 hour (–∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ backup frequency)

#### DR Drills

**Schedule:** Quarterly (–∫–æ–∂–Ω—ñ 3 –º—ñ—Å—è—Ü—ñ)

**Process:**

1. Schedule DR drill (announce 2 weeks ahead)
2. Execute DR procedures —É test environment
3. Measure RTO and RPO
4. Document issues
5. Update runbooks
6. Report to management

**Last DR drill:** 2026-01-15 (RTO: 1.5 hours, RPO: 30 min)

---

## Security Operations

### Certificate Management

#### Certificate Lifecycle

**Certificates used:**

- VoP Router TLS certificate (–ê–¶–°–ö)
- Client certificates (per bank, –ê–¶–°–ö)
- Internal certificates (Kubernetes, PostgreSQL)

**Renewal process:**

**90 days before expiry:**
1. Generate CSR (Certificate Signing Request)
   ```bash
   openssl req -new -key vop-router.key -out vop-router.csr
   ```

2. Submit CSR –¥–æ –ê–¶–°–ö
3. Wait for approval (3-5 business days)

**30 days before expiry:**
4. Receive new certificate –≤—ñ–¥ –ê–¶–°–¨–ö
5. Verify certificate:
   ```bash
   openssl x509 -in vop-router.crt -text -noout
   ```

**14 days before expiry:**
6. Create change request (RFC)
7. Schedule deployment

**7 days before expiry:**
8. Deploy new certificate (rolling update):
   ```bash
   # Update Kubernetes secret
   kubectl create secret tls vop-router-tls \
     --cert=vop-router.crt \
     --key=vop-router.key \
     --dry-run=client -o yaml | kubectl apply -f -

   # Restart pods to load new certificate
   kubectl rollout restart deployment/vop-router -n vop
   ```

9. Verify:
   ```bash
   echo | openssl s_client -connect vop-router.nbu.gov.ua:443 2>&1 | \
     openssl x509 -noout -dates
   ```

**Alerting:**

```yaml
- alert: CertificateExpiringSoon
  expr: (cert_expiry_timestamp - time()) < 7 * 24 * 3600
  severity: critical
  annotations:
    summary: "Certificate expires in < 7 days"
```

#### Certificate Revocation

**–Ø–∫—â–æ —Åertificate —Å–∫–æ–º–ø—Ä–æ–º–µ—Ç–æ–≤–∞–Ω–æ:**

1. **Immediate action:**
   - Revoke certificate —É –ê–¶–°–¨–ö
   - Update CRL (Certificate Revocation List)
   - Block certificate fingerprint —É VoP Router config

2. **Generate new certificate:**
   - New private key
   - New CSR
   - Submit –¥–æ –ê–¶–°–¨–ö (expedited process)

3. **Emergency deployment:**
   - Emergency RFC
   - Deploy ASAP (can be during business hours)

4. **Notify stakeholders:**
   - Email –≤—Å—ñ–º —É—á–∞—Å–Ω–∏–∫–∞–º VoP
   - Update –Ω–∞ website

**Estimated time:** 4-6 hours (–∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –ê–¶–°–¨–ö response time)

### Security Monitoring

**SIEM (Security Information and Event Management):**

–í—Å—ñ security events –Ω–∞–¥—Å–∏–ª–∞—é—Ç—å—Å—è –¥–æ SIEM system.

**Events monitored:**

- Failed authentication attempts
- Unauthorized access attempts
- Certificate errors
- Rate limit exceeded
- Suspicious patterns (e.g., brute force)
- Data exfiltration attempts

**Alerts:**

```yaml
# Failed authentication spike
- alert: FailedAuthSpike
  expr: rate(vop_router_auth_failures[5m]) > 10
  severity: warning

# Suspicious IP
- alert: SuspiciousIP
  expr: vop_router_requests_from_blacklisted_ip > 0
  severity: critical
```

**Weekly security review:**

- Review failed authentication logs
- Analyze traffic patterns
- Check for vulnerabilities (CVEs)
- Review access logs

### Vulnerability Management

**Process:**

1. **Weekly scan** (every Monday):
   ```bash
   # Container image scan
   trivy image vop-router:latest

   # Dependency scan
   npm audit
   pip-audit
   ```

2. **Vulnerability assessment:**
   - Critical: Fix within 24 hours
   - High: Fix within 7 days
   - Medium: Fix within 30 days
   - Low: Fix in next release

3. **Patch deployment:**
   - Create RFC
   - Test patch —É staging
   - Deploy to production

**CVE subscriptions:**

- GitHub Security Advisories
- NPM Security Alerts
- NIST NVD (National Vulnerability Database)

---

## Capacity Planning

### Metrics Collection

**–ó–±–∏—Ä–∞—Ç–∏ —â–æ–º—ñ—Å—è—Ü—è:**

- Peak requests per second
- Average requests per second
- Peak concurrent requests
- Database size (GB)
- Redis memory usage
- Storage usage (logs, backups)

**Dashboard:** https://monitoring.nbu.gov.ua/grafana/d/capacity-planning

### Growth Projections

**Historical data (6 months):**

| Month | Avg req/s | Peak req/s | Growth |
|-------|-----------|------------|--------|
| Jan   | 500       | 1,200      | -      |
| Feb   | 550       | 1,350      | +10%   |
| Mar   | 600       | 1,500      | +9%    |
| Apr   | 680       | 1,700      | +13%   |
| May   | 750       | 1,900      | +10%   |
| Jun   | 820       | 2,100      | +9%    |

**Average growth:** 10% per month

**Projection (next 6 months):**

- Jul: 900 req/s (peak 2,300)
- Aug: 990 req/s (peak 2,500)
- ...
- Dec: 1,450 req/s (peak 3,600)

### Capacity Thresholds

**Current capacity:**

- VoP Router: 2,000 req/s (with 3 pods)
- Database: 5,000 connections
- Redis: 10 GB memory

**Thresholds:**

| Resource | Warning (70%) | Critical (85%) | Action |
|----------|---------------|----------------|--------|
| VoP Router | 1,400 req/s | 1,700 req/s | Scale to 5 pods |
| Database connections | 3,500 | 4,250 | Increase pool size |
| Redis memory | 7 GB | 8.5 GB | Scale to larger instance |

**Alerting:**

```yaml
- alert: CapacityWarning
  expr: vop_router_rps > 1400
  for: 30m
  severity: warning
  annotations:
    summary: "VoP Router approaching capacity (70%)"
```

### Scaling Plan

**Short-term (< 1 month):**

- Horizontal scaling: Add more pods (3 ‚Üí 5 ‚Üí 10)
- Vertical scaling: Increase pod resources (2 CPU ‚Üí 4 CPU)

**Long-term (3-6 months):**

- Database sharding (—è–∫—â–æ > 1 TB)
- Multi-region deployment
- CDN for static content

**Budget:**

- Current: $5,000/month (infrastructure)
- Projected (Dec): $8,000/month (+60%)

---

## Runbooks

### Runbook Template

```markdown
# Runbook: [Task Name]

**Purpose:** What this runbook helps with
**Frequency:** Daily / Weekly / Monthly / As needed
**Owner:** Team / Person
**Estimated time:** X minutes

## Prerequisites

- Access to [system]
- Credentials for [service]
- Tools: [tool1], [tool2]

## Steps

1. **Step 1: [Description]**
   ```bash
   # Command
   kubectl get pods -n vop
   ```

   Expected output:
   ```
   NAME                          READY   STATUS
   vop-router-xxxxx              1/1     Running
   ```

2. **Step 2: [Description]**
   ...

## Verification

How to verify the task completed successfully.

## Rollback

If something goes wrong, how to rollback.

## Troubleshooting

Common issues and solutions.
```

### Common Runbooks

**1. Daily Health Check**

**Frequency:** Every day, 09:00 UTC

**Steps:**

1. Check Grafana dashboards
2. Review overnight alerts
3. Check backup status:
   ```bash
   aws s3 ls s3://nbu-vop-backups/postgres/ | tail -5
   ```
4. Check certificate expiry:
   ```bash
   kubectl get certificates -n vop
   ```
5. Review error logs (Kibana)

**Time:** 15 minutes

**2. Weekly Security Review**

**Frequency:** Every Monday, 10:00 UTC

**Steps:**

1. Run vulnerability scan
2. Review failed authentication logs
3. Check for CVEs
4. Review access logs
5. Update security dashboard

**Time:** 30 minutes

**3. Monthly Capacity Review**

**Frequency:** First Monday of month, 14:00 UTC

**Steps:**

1. Export metrics from Grafana
2. Update capacity planning spreadsheet
3. Calculate growth rate
4. Forecast next 3 months
5. Recommend scaling actions

**Time:** 1 hour

**4. Certificate Renewal**

**Frequency:** As needed (90 days before expiry)

**Steps:**

1. Generate CSR
2. Submit –¥–æ –ê–¶–°–¨–ö
3. Wait for approval
4. Receive certificate
5. Verify certificate
6. Create RFC
7. Schedule deployment
8. Deploy new certificate
9. Verify deployment

**Time:** 2-3 hours (spread over 2 weeks)

**5. Database Backup Restore Test**

**Frequency:** Weekly (Saturday 04:00 UTC)

**Steps:**

1. Download latest backup –≤—ñ–¥ S3
2. Restore to staging database
3. Verify data integrity
4. Run smoke tests
5. Document results
6. Delete staging data

**Time:** 1 hour

---

## –ö–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–∞ –µ—Å–∫–∞–ª–∞—Ü—ñ—è

### Contact Information

#### Operations Team (24/7)

| Role | Name | Phone | Email | Slack |
|------|------|-------|-------|-------|
| L1 Support (Shift 1) | Engineer A | +380-XX-XXX-XX01 | engineer-a@nbu.gov.ua | @engineer-a |
| L1 Support (Shift 2) | Engineer B | +380-XX-XXX-XX02 | engineer-b@nbu.gov.ua | @engineer-b |
| L2 Support (On-call) | Engineer C | +380-XX-XXX-XX03 | engineer-c@nbu.gov.ua | @engineer-c |

**Email:** vop-support@bank.gov.ua
**Slack:** #vop-support
**Phone hotline:** +380-44-XXX-XXXX

#### Engineering Team

| Role | Name | Phone | Email |
|------|------|-------|-------|
| Tech Lead | Engineer D | +380-XX-XXX-XX04 | tech-lead@nbu.gov.ua |
| Backend Engineer | Engineer E | +380-XX-XXX-XX05 | engineer-e@nbu.gov.ua |
| DevOps Engineer | Engineer F | +380-XX-XXX-XX06 | devops@nbu.gov.ua |

**Email:** vop-engineering@bank.gov.ua
**Slack:** #vop-engineering

#### Security Team

| Role | Name | Phone | Email |
|------|------|-------|-------|
| Security Lead | Security A | +380-XX-XXX-XX07 | security-lead@nbu.gov.ua |

**Email:** security@bank.gov.ua
**Slack:** #vop-security

#### Management

| Role | Name | Phone | Email |
|------|------|-------|-------|
| Head of IT | Manager A | +380-XX-XXX-XX08 | it-head@bank.gov.ua |
| VoP Product Owner | Manager B | +380-XX-XXX-XX09 | vop-po@nbu.gov.ua |

### Escalation Matrix

| Incident Severity | L1 Support | L2 Support | Tech Lead | Management |
|-------------------|------------|------------|-----------|------------|
| **P0 (Critical)** | Immediate | +15 min | +30 min | +1 hour |
| **P1 (High)** | Immediate | +30 min | +2 hours | +4 hours |
| **P2 (Medium)** | +1 hour | +4 hours | +1 day | - |
| **P3 (Low)** | +1 day | - | - | - |

**Escalation procedure:**

1. L1 Support —Å—Ç–≤–æ—Ä—é—î incident ticket
2. –Ø–∫—â–æ –Ω–µ –º–æ–∂–Ω–∞ –≤–∏—Ä—ñ—à–∏—Ç–∏ –∑–∞ [time] ‚Üí escalate –¥–æ L2
3. L2 –Ω–µ –º–æ–∂–µ –≤–∏—Ä—ñ—à–∏—Ç–∏ ‚Üí escalate –¥–æ Tech Lead
4. –Ø–∫—â–æ downtime > [threshold] ‚Üí notify Management

### Communication Templates

**Email template (incident notification):**

```
Subject: [SEVERITY] VoP Incident - [Brief description]

Dear stakeholders,

We are currently experiencing [issue description].

Impact:
- [Impact on users]
- [Affected systems]

Status: [Investigating / In progress / Resolved]

ETA: [Estimated time to resolution]

We will provide updates every [interval].

For questions, please contact: vop-support@bank.gov.ua

Regards,
VoP Operations Team
```

**Slack template (incident update):**

```
üî¥ P0 INCIDENT UPDATE

Incident ID: INC-20260207-001
Status: In Progress
Root cause: Database connection pool exhausted
Actions taken:
  ‚úÖ Restarted DB connections
  ‚úÖ Scaled to 5 replicas
  ‚è≥ Monitoring recovery

ETA: 10 minutes

Next update: 15:15 UTC
```

---

## –ü—ñ–¥—Å—É–º–æ–∫

–¶–µ–π –¥–æ–∫—É–º–µ–Ω—Ç –º—ñ—Å—Ç–∏—Ç—å –∫–ª—é—á–æ–≤—ñ –æ–ø–µ—Ä–∞—Ü—ñ–π–Ω—ñ –ø—Ä–æ—Ü–µ–¥—É—Ä–∏ –¥–ª—è VoP —Å–∏—Å—Ç–µ–º–∏:

‚úÖ **–ú–æ–Ω—ñ—Ç–æ—Ä–∏–Ω–≥ 24/7** ‚Äî Prometheus, Grafana, AlertManager, PagerDuty
‚úÖ **Incident Response** ‚Äî Severity levels, escalation, RCA
‚úÖ **Change Management** ‚Äî CAB process, change windows, emergency changes
‚úÖ **Backup —Ç–∞ Recovery** ‚Äî Daily backups, DR procedures, RTO/RPO
‚úÖ **Security Operations** ‚Äî Certificate management, vulnerability scanning, SIEM
‚úÖ **Capacity Planning** ‚Äî Growth forecasting, scaling thresholds
‚úÖ **Runbooks** ‚Äî Daily/weekly/monthly operational tasks

**–í–∞–∂–ª–∏–≤—ñ –Ω–∞–≥–∞–¥—É–≤–∞–Ω–Ω—è:**

- üìû On-call engineer –º–∞—î –≤—ñ–¥–ø–æ–≤—ñ—Å—Ç–∏ –Ω–∞ P0 incident –∑–∞ 15 —Ö–≤–∏–ª–∏–Ω
- üíæ Backups –ø–µ—Ä–µ–≤—ñ—Ä—è—é—Ç—å—Å—è —â–æ—Ç–∏–∂–Ω—è (restore test)
- üîê Certificates renewals –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑–∞ 90 –¥–Ω—ñ–≤ –¥–æ expiry
- üìä Capacity planning review ‚Äî —â–æ–º—ñ—Å—è—Ü—è
- üîÑ DR drill ‚Äî —â–æ–∫–≤–∞—Ä—Ç–∞–ª—É

---

**–í–µ—Ä—Å—ñ—è:** 1.0
**–î–∞—Ç–∞ –æ—Å—Ç–∞–Ω–Ω—å–æ–≥–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:** 2026-02-07
**–ù–∞—Å—Ç—É–ø–Ω–∏–π review:** Q2 2026

**–ö–æ–Ω—Ç–∞–∫—Ç–∏:**

- Operations: vop-support@bank.gov.ua
- Engineering: vop-engineering@bank.gov.ua
- Security: security@bank.gov.ua
- Emergency (24/7): +380-44-XXX-XXXX

**–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü—ñ—è:**

- Runbooks: https://docs.nbu.gov.ua/vop/runbooks
- Monitoring: https://monitoring.nbu.gov.ua/grafana
- Incident tracker: https://jira.nbu.gov.ua/projects/VOP
